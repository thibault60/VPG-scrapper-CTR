import streamlit as st
import pandas as pd
from serpapi import GoogleSearch
from concurrent.futures import ThreadPoolExecutor, as_completed
from io import BytesIO

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="VisibilitÃ© Voyage PrivÃ© â€“ SERP", layout="wide")

try:
    SERPAPI_KEY = st.secrets["serpapi_key"]
except Exception:
    st.error("âŒ ClÃ© SerpApi manquante dans `.streamlit/secrets.toml`.")
    st.stop()

VP_DOMAIN = "voyage-prive.com"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. RequÃªtes de test prÃ©dÃ©finies
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_QUERIES = """voyage en ThaÃ¯lande
sÃ©jour tout compris pas cher
hÃ´tel bord de mer"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Sidebar
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("âš™ï¸ ParamÃ¨tres")
    hl = st.selectbox("Langue (hl)", ["fr", "en", "es", "de", "it"], index=0)
    gl = st.selectbox("Pays (gl)", ["fr", "us", "es", "de", "it"], index=0)
    num_results = st.slider("RÃ©sultats organiques analysÃ©s", 10, 30, 10, step=10)
    max_workers = st.slider("Threads simultanÃ©s", 1, 8, 3)

    st.markdown("---")
    st.markdown(
        "**Sources extraites**\n\n"
        "- ğŸ”µ Lien bleu principal (rÃ©sultat organique VP)\n"
        "- ğŸ  Sitelinks carrousel (offres VP sous le rÃ©sultat)"
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Zone de saisie des requÃªtes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ” VisibilitÃ© Voyage PrivÃ© â€“ Liens organiques & Sitelinks")
st.markdown(
    "Extrait les **URLs `voyage-prive.com`** depuis :\n"
    "- ğŸ”µ Le **lien bleu principal** dans les rÃ©sultats organiques\n"
    "- ğŸ  Les **sitelinks en carrousel** (offres affichÃ©es sous le rÃ©sultat principal)"
)

queries_raw = st.text_area(
    "ğŸ“‹ Liste de requÃªtes (une par ligne)",
    value=DEFAULT_QUERIES,
    height=160,
)
queries = [q.strip() for q in queries_raw.splitlines() if q.strip()]
st.caption(f"**{len(queries)} requÃªte(s)** chargÃ©e(s)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Extraction SerpApi
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_vp_results(query: str, hl: str, gl: str, num: int) -> list[dict]:
    """
    Extrait depuis SerpApi :
    - Le rÃ©sultat organique principal VP (lien bleu)
    - Les sitelinks inline VP (carrousel d'offres sous le rÃ©sultat)
    """
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "hl": hl,
        "gl": gl,
        "num": num,
        "engine": "google",
    }

    try:
        search = GoogleSearch(params)
        data = search.get_dict()
    except Exception as exc:
        return [_row(query, "âš ï¸ Erreur API", "â€”", str(exc), "", "â€”")]

    rows = []
    organic = data.get("organic_results", [])

    for pos, result in enumerate(organic, start=1):
        main_link = result.get("link", "")

        # â”€â”€ ğŸ”µ Lien bleu principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if VP_DOMAIN in main_link:
            rows.append(_row(
                query   = query,
                type_   = "ğŸ”µ Lien principal",
                position= pos,
                titre   = result.get("title", "â€”"),
                url     = main_link,
                snippet = result.get("snippet", "â€”"),
            ))

            # â”€â”€ ğŸ  Sitelinks inline (carrousel offres) â”€
            # SerpApi retourne les sitelinks dans organic_results[n]["sitelinks"]
            # sous la clÃ© "inline" (liste de dicts avec "title" et "link")
            sitelinks_data = result.get("sitelinks", {})

            if isinstance(sitelinks_data, dict):
                inline_links = sitelinks_data.get("inline", [])
            elif isinstance(sitelinks_data, list):
                inline_links = sitelinks_data
            else:
                inline_links = []

            for idx, sl in enumerate(inline_links, start=1):
                sl_link = sl.get("link", "")
                if VP_DOMAIN in sl_link:
                    rows.append(_row(
                        query   = query,
                        type_   = "ğŸ  Sitelink carrousel",
                        position= f"{pos}.{idx}",
                        titre   = sl.get("title", "â€”"),
                        url     = sl_link,
                        snippet = sl.get("snippet", ""),
                    ))

    if not rows:
        rows.append(_row(query, "âŒ Absent", "â€”", "Voyage PrivÃ© absent des rÃ©sultats", "", ""))

    return rows


def _row(query, type_, position, titre, url, snippet) -> dict:
    return {
        "RequÃªte" : query,
        "Type"    : type_,
        "Position": position,
        "Titre"   : titre,
        "URL"     : url,
        "Snippet" : snippet,
    }


@st.cache_data(ttl=3_600, show_spinner=False)
def run_all(queries_tuple: tuple, hl: str, gl: str, num: int, workers: int) -> pd.DataFrame:
    rows = []
    progress = st.progress(0.0, text="ğŸ”„ Analyse des SERPâ€¦")
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(extract_vp_results, q, hl, gl, num): q
            for q in queries_tuple
        }
        total = len(futures)
        for i, future in enumerate(as_completed(futures), 1):
            rows.extend(future.result())
            progress.progress(i / total, text=f"ğŸ”„ {i}/{total} requÃªtes analysÃ©esâ€¦")
    progress.empty()
    return pd.DataFrame(rows)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. Lancement + Affichage
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if st.button("ğŸš€ Lancer l'extraction", type="primary", disabled=len(queries) == 0):

    df = run_all(tuple(queries), hl, gl, num_results, max_workers)

    # â”€â”€ KPIs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    present   = df[~df["Type"].isin(["âŒ Absent", "âš ï¸ Erreur API"])]
    principal = df[df["Type"] == "ğŸ”µ Lien principal"]
    sitelinks = df[df["Type"] == "ğŸ  Sitelink carrousel"]
    absent    = df[df["Type"] == "âŒ Absent"]

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("RequÃªtes analysÃ©es",     len(queries))
    c2.metric("ğŸ”µ Liens principaux VP", len(principal))
    c3.metric("ğŸ  Sitelinks offres VP", len(sitelinks))
    c4.metric("âŒ Sans prÃ©sence VP",    len(absent))

    st.markdown("---")

    # â”€â”€ Tableau principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("ğŸ“Š RÃ©sultats dÃ©taillÃ©s")

    type_filter = st.multiselect(
        "Filtrer par type",
        options=df["Type"].unique().tolist(),
        default=df["Type"].unique().tolist(),
    )
    df_filtered = df[df["Type"].isin(type_filter)]

    st.dataframe(
        df_filtered,
        use_container_width=True,
        height=450,
        column_config={
            "URL": st.column_config.LinkColumn("URL", display_text="ğŸ”— Voir"),
        },
        column_order=["RequÃªte", "Type", "Position", "Titre", "URL", "Snippet"],
    )

    # â”€â”€ Vue groupÃ©e par requÃªte â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    st.subheader("ğŸ” DÃ©tail par requÃªte")

    for query in df["RequÃªte"].unique():
        subset  = df[df["RequÃªte"] == query]
        nb_sl   = len(subset[subset["Type"] == "ğŸ  Sitelink carrousel"])
        has_main= len(subset[subset["Type"] == "ğŸ”µ Lien principal"]) > 0
        has_vp  = has_main or nb_sl > 0

        badge_main = "ğŸ”µ lien principal" if has_main else ""
        badge_sl   = f"+ ğŸ  {nb_sl} sitelink(s)" if nb_sl else ""
        label = f"{'âœ…' if has_vp else 'âŒ'} {query} â€” {badge_main} {badge_sl}".strip(" â€”")

        with st.expander(label):
            if not has_vp:
                st.info("Voyage PrivÃ© n'apparaÃ®t pas dans les rÃ©sultats analysÃ©s.")
                continue

            # Lien principal
            main_rows = subset[subset["Type"] == "ğŸ”µ Lien principal"]
            if not main_rows.empty:
                r = main_rows.iloc[0]
                st.markdown(f"**ğŸ”µ RÃ©sultat principal â€” Position `{r['Position']}`**")
                st.markdown(f"**{r['Titre']}**")
                st.markdown(f"[{r['URL']}]({r['URL']})")
                if r["Snippet"] and r["Snippet"] not in ("â€”", ""):
                    st.caption(r["Snippet"])
                st.markdown("---")

            # Sitelinks carrousel
            sl_rows = subset[subset["Type"] == "ğŸ  Sitelink carrousel"]
            if not sl_rows.empty:
                st.markdown(f"**ğŸ  Sitelinks carrousel â€” {len(sl_rows)} offre(s)**")
                for _, r in sl_rows.iterrows():
                    col_a, col_b = st.columns([3, 7])
                    col_a.markdown(f"Position `{r['Position']}`")
                    col_b.markdown(f"**{r['Titre']}** â†’ [{r['URL']}]({r['URL']})")

    # â”€â”€ Exports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    col1, col2 = st.columns(2)

    csv = df.to_csv(index=False).encode("utf-8")
    col1.download_button(
        "ğŸ’¾ TÃ©lÃ©charger CSV",
        data=csv,
        file_name="vp_serp_sitelinks.csv",
        mime="text/csv",
    )

    xlsx_buffer = BytesIO()
    with pd.ExcelWriter(xlsx_buffer, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="DÃ©tail")
        summary = (
            df[~df["Type"].isin(["âŒ Absent", "âš ï¸ Erreur API"])]
            .groupby(["RequÃªte", "Type"])
            .size()
            .reset_index(name="Nombre URLs VP")
        )
        summary.to_excel(writer, index=False, sheet_name="RÃ©sumÃ©")
    xlsx_buffer.seek(0)
    col2.download_button(
        "ğŸ“Š TÃ©lÃ©charger XLSX",
        data=xlsx_buffer,
        file_name="vp_serp_sitelinks.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )
